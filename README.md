# Sentiment_Analysis

 * [한국어 감성 분석에서 토큰 단위와 불용어 처리 기준에 따른 성능 비교](http://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE09301944)
 
   * `정영석`, `허탁성`, `김유섭`

-------------------------------------------

## 데이터

 * [[NSMC](https://github.com/e9t/nsmc)](Naver Sentiment Movie Corpus)
 
    * `id`, `document`, `label`으로 총 20만 문장 구성 (`label`은 긍정, 부정의 2개의 Class)
 
    * ratings_train.txt - 15만 문장
    
    * ratings_test.txt - 5만 문장http://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE09301944)] 
 
 -------------------------------------------
 
## 실험 방법
 
 * Artificial Meural Network을 이용한 성능 비교
  
   * Convolutional Neural Networks(CNN), Long Short-Term Memory(LSTM), LSTM + CNN을 사용
  
 * 토큰 단위
 
   * 어절, 형태소, 음절, 음소
  
 * 불용어 처리 (전처리)
 
   * [인터넷 정의 불용어 리스트](https://bab2min.tistory.com/544)
   * 조사 제거
   * 특수문자와 한자 제거
   * 명사만 추출
   * NSMC에서 나타나는 가장 높은 빈도수 100개의 형태소 제거

-------------------------------------------
   
## 실험 결과

 * **실험 결과의 모든 값은 5회 실험 후 평균을 낸 값**

   * **토큰 단위 비교**
   
     * Convolutional Neural Networks

       |    CNN    | 어절  | 형태소 | 음절  | 음소  |
       | :-------: | :---: | :----: | :---: | :---: |
       | Accuracy  | 0.785 | 0.855  | 0.846 | 0.794 |
       | Loss      | 0.158 | 0.241  | 0.278 | 0.411 |
       | F1_Score  | 0.79  | 0.856  | 0.846 | 0.785 |   

     * Long Short-Term Memory

       |    LSTM   | 어절  | 형태소 | 음절  | 음소  |
       | :-------: | :---: | :----: | :---: | :---: |
       | Accuracy  | 0.778 | 0.852  | 0.84  | 0.802 |
       | Loss      | 0.13  | 0.335  | 0.301 | 0.36  |
       | F1_Score  | 0.783 | 0.854  | 0.838 | 0.809 |      

     * Long Short-Term Memory + Convolutional Neural Networks

       | LSTM+CNN  | 어절  | 형태소 | 음절  | 음소  |
       | :-------: | :---: | :----: | :---: | :---: |
       | Accuracy  | 0.772 | 0.856  | 0.843 | 0.838 |
       | Loss      | 0.143 | 0.383  | 0.403 | 0.29  |
       | F1_Score  | 0.766 | 0.856  | 0.842 | 0.838 |   



   * **불용어 처리 비교**
   
     * Convolutional Neural Networks

       |    CNN    | 불용어 리스트 | 조사 제거 | 특수문자 및 한자 | 명사 추출 | NSMC불용어 |
       | :-------: | :----------: | :------: | :------------: | :-------: | :-------: |   
       | Accuracy  | 0.852        | 0.854    | 0.853          | 0.769     | 0.853     |
       | Loss      | 0.245        | 0.239    | 0.224          | 0.405     | 0.224     |
       | F1_Score  | 0.853        | 0.854    | 0.853          | 0.773     | 0.853     |   

     * Long Short-Term Memory

       |    LSTM   | 불용어 리스트 | 조사 제거 | 특수문자 및 한자 | 명사 추출 | NSMC불용어 |
       | :-------: | :----------: | :------: | :------------: | :-------: | :-------: |
       | Accuracy  | 0.85         | 0.85     | 0.849          | 0.766     | 0.849     |
       | Loss      | 0.242        | 0.24     | 0.242          | 0.415     | 0.242     |
       | F1_Score  | 0.851        | 0.853    | 0.85           | 0.77      | 0.85      |   
     
     * Long Short-Term Memory + Convolutional Neural Networks

       |LSTM + CNN | 불용어 리스트 | 조사 제거 | 특수문자 및 한자 | 명사 추출 | NSMC불용어 |
       | :-------: | :----------: | :------: | :------------: | :-------: | :-------: |  
       | Accuracy  | 0.852        | 0.852    | 0.851          | 0.765     | 0.851     |
       | Loss      | 0.231        | 0.239    | 0.234          | 0.389     | 0.234     |
       | F1_Score  | 0.852        | 0.852    | 0.851          | 0.775     | 0.851     |   
  
-------------------------------------------

 ## 결론
 
  * 불용어 처리의 방법보다 토큰 단위가 성능에 많은 영향을 미침.
